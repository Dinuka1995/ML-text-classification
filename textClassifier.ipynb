{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "filepath='trainset.txt'   #file_path\n",
    "names=['class','title','date','body']   # headers\n",
    "dataset = pd.read_csv(filepath,delimiter=\"\\t\", names = names) # read the file \n",
    "#print('Shape o data : ', dataset.shape)\n",
    "#print(dataset.describe())\n",
    "labels=dataset['class']\n",
    "dataset=dataset.drop(['date','class'], axis=1)\n",
    "\n",
    "dataset['body']=dataset['body'].str.lower()\n",
    "dataset['title']=dataset['title'].str.lower()\n",
    "body=dataset['body']\n",
    "title=dataset['title']\n",
    "\n",
    "import re\n",
    "from nltk.stem import PorterStemmer \n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "ps = PorterStemmer() \n",
    "\n",
    "\n",
    "def preprocessor(dataset):\n",
    "    for i in range(len(dataset['body'])-1):\n",
    "        dataset['body'][i]=regex.sub(' ',(dataset['body'][i]))\n",
    "        dataset['body'][i]=word_tokenize(dataset['body'][i])\n",
    "        temp_words=\"\"\n",
    "        for w in dataset['body'][i]:\n",
    "            if w not in stop_words:\n",
    "                w=ps.stem(w)\n",
    "                temp_words=temp_words+\" \"+w\n",
    "        dataset['body'][i]=temp_words\n",
    "    \n",
    "    for i in range(len(dataset['title'])-1):\n",
    "        dataset['title'][i]=regex.sub(' ',(dataset['title'][i]))\n",
    "        dataset['title'][i]=word_tokenize(dataset['title'][i])\n",
    "        temp_words=\"\"\n",
    "        for w in dataset['title'][i]:\n",
    "            if w not in stop_words:\n",
    "                w=ps.stem(w)\n",
    "                #temp_words.append(w)\n",
    "                temp_words=temp_words+\" \"+w\n",
    "        dataset['title'][i]=temp_words\n",
    "    dataset['data'] = dataset[['title', 'body']].apply(lambda x: ' '.join(x), axis=1)\n",
    "    dataset=dataset.drop(['body','title'], axis=1)\n",
    "    return dataset\n",
    "\n",
    "#making new column with Title and Body\n",
    "dataset=preprocessor(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'),lowercase=True)\n",
    "X = cv.fit_transform(dataset['data'])\n",
    "#print(dataset['data'])\n",
    "y=labels\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1) \n",
    "\n",
    "X_train=X_train.toarray()\n",
    "X_test=X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes model accuracy(in %): 95.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB \n",
    "gnb = GaussianNB() \n",
    "gnb.fit(X_train, y_train) \n",
    "\n",
    "y_pred = gnb.predict(X_test) \n",
    "#print(y_pred)\n",
    "#print(y_test)\n",
    "from sklearn import metrics \n",
    "print(\"Gaussian Naive Bayes model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath='testsetwithoutlabels.txt'   #file_path\n",
    "names=['title','date','body']\n",
    "dataset_pred = pd.read_csv(filepath,delimiter=\"\\t\", names = names) # read the file \n",
    "#print(dataset_pred)\n",
    "dataset_pred=dataset_pred.drop(['date'], axis=1)\n",
    "dataset_pred=preprocessor(dataset_pred)\n",
    "X_pred = cv.transform(dataset_pred['data'])\n",
    "#print(dataset_pred['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_pred.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1 -1 -1  1 -1 -1  1 -1 -1 -1  1  1 -1  1  1 -1  1  1 -1 -1 -1 -1\n",
      " -1 -1  1 -1 -1 -1 -1  1 -1  1 -1  1 -1  1 -1 -1 -1  1 -1 -1  1 -1  1 -1\n",
      "  1 -1  1  1 -1 -1  1 -1  1  1  1 -1 -1 -1 -1 -1  1 -1  1  1  1 -1  1 -1\n",
      " -1 -1  1 -1  1  1  1 -1  1 -1  1 -1  1 -1 -1  1  1  1  1  1  1  1  1 -1\n",
      " -1  1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "y_pred=gnb.predict(X_pred.toarray())\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
